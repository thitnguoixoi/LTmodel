{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfeaceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "data = []\n",
    "for file in glob.glob(\"*.json\"):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data.extend(json.load(f))\n",
    "\n",
    "print(f\"Đã đọc tổng cộng {len(data)} bản ghi từ các file JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2708aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_request_keys = set()\n",
    "all_headers_keys = set()\n",
    "\n",
    "for obj in data:\n",
    "    req = obj.get(\"request\", {})\n",
    "    if isinstance(req, dict):\n",
    "        all_request_keys.update([k for k in req.keys() if k != \"headers\"])\n",
    "        headers = req.get(\"headers\", {})\n",
    "        if isinstance(headers, dict):\n",
    "            all_headers_keys.update(headers.keys())\n",
    "\n",
    "print(\"Tất cả key của request:\", list(all_request_keys))\n",
    "print(\"Tất cả key của request['headers']:\", list(all_headers_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Tạo danh sách các cột với tiền tố phù hợp\n",
    "columns = [f\"request.headers.{k}\" for k in all_headers_keys] + [f\"request.{k}\" for k in all_request_keys]\n",
    "\n",
    "with open(\"Alldata.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for obj in data:\n",
    "        row = {}\n",
    "        req = obj.get(\"request\", {})\n",
    "        if isinstance(req, dict):\n",
    "            # Lấy các key của request với tiền tố \"request.\"\n",
    "            for key in all_request_keys:\n",
    "                row[f\"request.{key}\"] = req.get(key, \"\")\n",
    "            # Xử lý phần headers với tiền tố \"request.headers.\"\n",
    "            headers = req.get(\"headers\", {})\n",
    "            if isinstance(headers, dict):\n",
    "                for key in all_headers_keys:\n",
    "                    row[f\"request.headers.{key}\"] = headers.get(key, \"\")\n",
    "            else:\n",
    "                for key in all_headers_keys:\n",
    "                    row[f\"request.headers.{key}\"] = \"\"\n",
    "        else:\n",
    "            for key in all_request_keys:\n",
    "                row[f\"request.{key}\"] = \"\"\n",
    "            for key in all_headers_keys:\n",
    "                row[f\"request.headers.{key}\"] = \"\"\n",
    "        # Nếu giá trị của request.Attack_Tag rỗng (hoặc không có) thì gán là \"Normal\"\n",
    "        if f\"request.Attack_Tag\" in row:\n",
    "            if row[f\"request.Attack_Tag\"] is None or row[f\"request.Attack_Tag\"] == \"\" or str(row[f\"request.Attack_Tag\"]).lower() == \"nan\":\n",
    "                row[f\"request.Attack_Tag\"] = \"Normal\"\n",
    "        else:\n",
    "            row[f\"request.Attack_Tag\"] = \"Normal\"\n",
    "            \n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"CSV file has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828e1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanlo\\AppData\\Local\\Temp\\ipykernel_15208\\3794374857.py:2: DtypeWarning: Columns (0,1,5,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"dataset\\\\Alldata.csv\", encoding=\"utf-8\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset\\\\Alldata.csv\", encoding=\"utf-8\")\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a08a44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng nhãn 'Normal': 364091\n",
      "Số lượng các nhãn khác: 67302\n"
     ]
    }
   ],
   "source": [
    "# Đếm số lượng nhãn Normal và các nhãn khác trong cột Attack_Tag\n",
    "counts = df['request.Attack_Tag'].value_counts()\n",
    "normal_count = counts.get(\"Normal\", 0)\n",
    "other_count = df['request.Attack_Tag'].size - normal_count\n",
    "\n",
    "print(\"Số lượng nhãn 'Normal':\", normal_count)\n",
    "print(\"Số lượng các nhãn khác:\", other_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f622d131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu 65000 mẫu 'Normal' và 65000 mẫu 'các nhãn khác' vào file alldata_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "# Lọc dữ liệu theo nhãn\n",
    "df_normal = df[df['request.Attack_Tag'] == \"Normal\"]\n",
    "df_other = df[df['request.Attack_Tag'] != \"Normal\"]\n",
    "\n",
    "# Lấy mẫu\n",
    "sample_normal = df_normal.sample(n=65000, random_state=42)\n",
    "sample_other = df_other.sample(n=65000, random_state=42)\n",
    "\n",
    "# Kết hợp\n",
    "df_sample = pd.concat([sample_normal, sample_other], ignore_index=True)\n",
    "\n",
    "df_sample = df_sample.fillna(\"nan\").replace(\"\", \"nan\")\n",
    "\n",
    "# Ghi ra file\n",
    "df_sample.to_csv(\"alldata_balanced.csv\", index=False)\n",
    "\n",
    "print(\"Đã lưu 65000 mẫu 'Normal' và 65000 mẫu 'các nhãn khác' vào file alldata_balanced.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63d5518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanlo\\AppData\\Local\\Temp\\ipykernel_15768\\955098165.py:4: DtypeWarning: Columns (13,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"dataset\\\\alldata_balanced.csv\", encoding=\"utf-8\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df = pd.read_csv(\"dataset\\\\alldata_balanced.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34cb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import requests\n",
    "import math\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import base64\n",
    "from urllib.parse import unquote\n",
    "\n",
    "\n",
    "class HTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the HTokenizer with a specified regex pattern.\n",
    "\n",
    "        :param pattern: A regex pattern to tokenize the text. Default is r'\\w+' which matches words.\n",
    "        \"\"\"\n",
    "        self.tld_list = self.load_tld_list()\n",
    "        self.tokenizer = {\n",
    "            \"user-agent\": RegexpTokenizer(r'[A-Za-z0-9]+'),\n",
    "            \"date\": RegexpTokenizer(r'\\d{4}-\\d{2}-\\d{2}|\\d{2}:\\d{2}:\\d{2}|[A-Za-z0-9]+'),\n",
    "            \"host\": RegexpTokenizer(r'[A-Za-z0-9\\.]+|\\:'),\n",
    "            \"default\": RegexpTokenizer(r'[a-zA-Z0-9]+'),\n",
    "            \"content-length\": RegexpTokenizer(r'[0-9\\.]+'),\n",
    "            \"attack\": RegexpTokenizer(\n",
    "                r'(?:ldap|rmi|ldaps|dns|iiop|nis|http|https|ftp|file)(?=[a-z])'\n",
    "                r'|\\.\\./|\\.\\.\\\\|:\\/\\/'\n",
    "                r'|[a-zA-Z0-9_.-]+\\.(?:txt|csv|log|exe|py|php|html?|json|js|xml|conf|sh|bat|dll|zip|tar|gz|pdf|docx|doc|pptx|xlsx|ppt|xls|docm|dotx|dotm|potx|potm|ppsx|ppsm|pps|dot|jpg|jpeg|png|gif|bmp|tiff|tif|svg|webp|mp4|avi|mov|mkv|wmv|flv|mp3|wav|ogg|aac|wma|m4a|aspx)'\n",
    "                r'|\\(|\\)|\\;|\\=|\\:|\\,|\\&|\\%|\\\\|\\/|\\+|\\?|\\#|\\[|\\]|\\{|\\}|\\~'\n",
    "                r'|\\d{2}:\\d{2}:\\d{2}'\n",
    "                r'|[A-Za-z0-9\\.]+'\n",
    "                r'|[A-Za-z0-9]+'\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def load_tld_list(self):\n",
    "        url = \"https://data.iana.org/TLD/tlds-alpha-by-domain.txt\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        lines = response.text.splitlines()\n",
    "        tlds = set(line.lower() for line in lines if not line.startswith(\"#\"))\n",
    "        return tlds\n",
    "\n",
    "    def calculate_entropy(self, s):\n",
    "        \"\"\"\n",
    "        Tính entropy trung bình của chuỗi (đo độ ngẫu nhiên).\n",
    "        \"\"\"\n",
    "        if not s:\n",
    "            return 0\n",
    "        counter = Counter(s)\n",
    "        total = len(s)\n",
    "        entropy = -sum((count/total) * math.log2(count/total)\n",
    "                       for count in counter.values())\n",
    "        return entropy\n",
    "\n",
    "    def is_random_string(self, s, entropy_threshold=3.5):\n",
    "        \"\"\"\n",
    "        Nhận biết chuỗi có phải chuỗi ngẫu nhiên hay không.\n",
    "\n",
    "        - entropy_threshold: càng cao thì độ ngẫu nhiên càng lớn (giá trị 4.0 là hợp lý)\n",
    "        \"\"\"\n",
    "\n",
    "        s = s.strip()\n",
    "        if not s:\n",
    "            return False\n",
    "        # 1. Nếu ký tự đầu là chữ cái và lặp lại liên tiếp → random\n",
    "        first_char = s[0]\n",
    "        if first_char.isalpha():  # chỉ xét nếu là chữ cái a-z, A-Z\n",
    "            repeat_count = 1\n",
    "            for i in range(1, len(s)):\n",
    "                if s[i] == first_char:\n",
    "                    repeat_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            if repeat_count >= 2:\n",
    "                return True\n",
    "\n",
    "        # 2. Nếu chứa cả chữ và số (không phải chỉ toàn số) thì đánh dấu là random\n",
    "        if re.search(r\"[A-Za-z]\", s) and re.search(r\"\\d\", s):\n",
    "            return True\n",
    "\n",
    "        # 3. Tính entropy\n",
    "        entropy = self.calculate_entropy(s)\n",
    "\n",
    "        # 4. Nếu chứa nhiều ký tự đặc biệt và entropy cao => random\n",
    "        special_chars = set(s) - set(string.ascii_letters + string.digits)\n",
    "        if entropy >= entropy_threshold and len(special_chars) >= 2:\n",
    "            return True\n",
    "\n",
    "        # 5. Nếu entropy cao và có nhiều loại ký tự khác nhau => khả năng random cao\n",
    "        if entropy >= entropy_threshold:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def is_file_path(self, s):\n",
    "        \"\"\"\n",
    "        Nhận biết chuỗi là file path hoặc tên file (Windows, Unix, hoặc tên file đơn lẻ).\n",
    "        \"\"\"\n",
    "        # Pattern kiểm tra path hoặc file có phần mở rộng\n",
    "        pattern = re.compile(\n",
    "            r\"\"\"(\n",
    "                (?:[a-zA-Z]:\\\\|/)?                     # ổ đĩa hoặc bắt đầu với /\n",
    "                # thư mục (có thể có nhiều cấp)\n",
    "                (?:[\\w\\-\\. ]+[\\\\/])*\n",
    "                [\\w\\-\\. ]+\\.(?:txt|csv|log|exe|py|php|html?|json|js|xml|conf|sh|bat|dll|zip|tar|gz|pdf|docx|doc|pptx|xlsx|ppt|xls|docm|dotx|dotm|potx|potm|ppsx|ppsm|pps|dot|jpg|jpeg|png|gif|bmp|tiff|tif|svg|webp|mp4|avi|mov|mkv|wmv|flv|mp3|wav|ogg|aac|wma|m4a|aspx?)\n",
    "            )$\"\"\",\n",
    "            re.VERBOSE | re.IGNORECASE\n",
    "        )\n",
    "        return bool(pattern.match(s))\n",
    "\n",
    "    def is_domain(self, s):\n",
    "        \"\"\"\n",
    "        Kiểm tra xem chuỗi s có phải là tên miền hợp lệ không.\n",
    "        \"\"\"\n",
    "        if not s or not isinstance(s, str):\n",
    "            return False\n",
    "\n",
    "        # Loại bỏ dấu chấm đầu (.) hoặc tiền tố như www.\n",
    "        s = s.lstrip('.').lower()\n",
    "        s = re.sub(r'^www\\.', '', s)\n",
    "\n",
    "        # Regex cơ bản kiểm tra định dạng tên miền\n",
    "        pattern = re.compile(r\"^(?!:\\/\\/)([a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}$\")\n",
    "        match = pattern.match(s)\n",
    "        if not match:\n",
    "            return False\n",
    "\n",
    "        # Lấy tất cả phần sau mỗi dấu chấm và thử kiểm tra TLD\n",
    "        parts = s.split('.')\n",
    "        for i in range(len(parts)):\n",
    "            potential_tld = '.'.join(parts[i:])\n",
    "            if potential_tld in self.tld_list:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def decode_base64(self, encoded_str):\n",
    "        \"\"\"\n",
    "        Attempt to decode a base64 encoded string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if string is likely base64\n",
    "            if not re.match(r'^[A-Za-z0-9+/=]+$', encoded_str):\n",
    "                return encoded_str\n",
    "\n",
    "            # Decode the base64 content\n",
    "            decoded = base64.b64decode(encoded_str)\n",
    "\n",
    "            # Try to decode as UTF-8\n",
    "            try:\n",
    "                return decoded.decode('utf-8', errors='replace')\n",
    "            except UnicodeDecodeError:\n",
    "                # If it can't be decoded as text, return None\n",
    "                return encoded_str\n",
    "        except:\n",
    "            return encoded_str\n",
    "\n",
    "    def clean_tokens(self, tokens):\n",
    "        \"\"\"Add a token to the normal tokens set and increment its counter.\n",
    "\n",
    "        Args:\n",
    "            token: Token string to add\n",
    "        \"\"\"\n",
    "        new_tokens = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            if re.fullmatch(r\"\\d+x\\d+\", token):\n",
    "                token = \"<res>\"\n",
    "            elif re.fullmatch(r\"(?:\\d{1,3}\\.){3}\\d{1,3}\", token):\n",
    "                token = (\"<ip>\")\n",
    "            if re.fullmatch(r\"[\\d\\.]+\", token):\n",
    "                token = \"<num>\"\n",
    "            elif re.fullmatch(r\"\\d{2}:\\d{2}:\\d{2}\", token):\n",
    "                token = \"<time>\"\n",
    "            elif token.lstrip('-').isdigit():\n",
    "                token = \"<num>\"\n",
    "            elif self.is_file_path(token):\n",
    "                token = \"<file>\"\n",
    "            elif self.is_domain(token):\n",
    "                token = \"<domain>\"\n",
    "            elif token == '../' or token == '..\\\\':\n",
    "                new_tokens.append(token)\n",
    "                continue\n",
    "            elif self.is_random_string(token):\n",
    "                token = \"<random>\"\n",
    "            elif '.' in token and not self.is_file_path(token) and not self.is_domain(token):\n",
    "                token = \"<random>\"\n",
    "            new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def process_user(self, value):\n",
    "        \"\"\"\n",
    "        Process user-agent strings to extract tokens.\n",
    "        \"\"\"\n",
    "        # print(f\"User-Agent: {value}\")\n",
    "        decoded = unquote(str(value))\n",
    "        decoded = decoded.replace(\"--\", \" \")\n",
    "        decoded = decoded.replace(\"_\", \" \")\n",
    "        decoded = decoded.replace(\"'\", \"\")\n",
    "        decoded = decoded.replace(\"[\", \"\")\n",
    "        decoded = decoded.replace(\"]\", \"\")\n",
    "        # print(f\"Decoded User-Agent: {decoded}\")\n",
    "        tokens = self.tokenizer[\"host\"].tokenize(decoded)\n",
    "        # print(\"Tokens before clean:\", tokens)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = self.clean_tokens(tokens)\n",
    "        # print(\"Tokens after clean:\", tokens)\n",
    "        return tokens\n",
    "    def process_date(self, value):\n",
    "        \"\"\"\n",
    "        Process user-agent strings to extract tokens.\n",
    "        \"\"\"\n",
    "        # print(f\"User-Agent: {value}\")\n",
    "        decoded = unquote(str(value))\n",
    "        decoded = decoded.replace(\"--\", \" \")\n",
    "        decoded = decoded.replace(\"_\", \" \")\n",
    "        decoded = decoded.replace(\"'\", \"\")\n",
    "        decoded = decoded.replace(\"[\", \"\")\n",
    "        decoded = decoded.replace(\"]\", \"\")\n",
    "        # print(f\"Decoded User-Agent: {decoded}\")\n",
    "        tokens = self.tokenizer[\"date\"].tokenize(decoded)\n",
    "        # print(\"Tokens before clean:\", tokens)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = self.clean_tokens(tokens)\n",
    "        # print(\"Tokens after clean:\", tokens)\n",
    "        return tokens\n",
    "    def process_host(self, value):\n",
    "        \"\"\"\n",
    "        Process user-agent strings to extract tokens.\n",
    "        \"\"\"\n",
    "        # print(f\"User-Agent: {value}\")\n",
    "        decoded = unquote(str(value))\n",
    "        decoded = decoded.replace(\"--\", \" \")\n",
    "        decoded = decoded.replace(\"_\", \" \")\n",
    "        decoded = decoded.replace(\"'\", \"\")\n",
    "        decoded = decoded.replace(\"[\", \"\")\n",
    "        decoded = decoded.replace(\"]\", \"\")\n",
    "        # print(f\"Decoded User-Agent: {decoded}\")\n",
    "        tokens = self.tokenizer[\"host\"].tokenize(decoded)\n",
    "        # print(\"Tokens before clean:\", tokens)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = self.clean_tokens(tokens)\n",
    "        # print(\"Tokens after clean:\", tokens)\n",
    "        return tokens\n",
    "    def process_content_length(self, value):\n",
    "        \"\"\"\n",
    "        Process content-length strings to extract tokens.\n",
    "        \"\"\"\n",
    "        # print(f\"Content-Length: {value}\")\n",
    "        decoded = unquote(str(value))\n",
    "        decoded = decoded.replace(\"--\", \" \")\n",
    "        decoded = decoded.replace(\"_\", \" \")\n",
    "        decoded = decoded.replace(\"'\", \"\")\n",
    "        decoded = decoded.replace(\"[\", \"\")\n",
    "        decoded = decoded.replace(\"]\", \"\")\n",
    "        # print(f\"Decoded Content-Length: {decoded}\")\n",
    "        tokens = self.tokenizer[\"content-length\"].tokenize(decoded)\n",
    "        # print(\"Tokens before clean:\", tokens)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = self.clean_tokens(tokens)\n",
    "        # print(\"Tokens after clean:\", tokens)\n",
    "        return tokens\n",
    "    def process_default(self, value):\n",
    "        \"\"\"\n",
    "        Process user-agent strings to extract tokens.\n",
    "        \"\"\"\n",
    "        # print(f\"User-Agent: {value}\")\n",
    "        decoded = unquote(str(value))\n",
    "        decoded = decoded.replace(\"--\", \" \")\n",
    "        decoded = decoded.replace(\"_\", \" \")\n",
    "        decoded = decoded.replace(\"'\", \"\")\n",
    "        decoded = decoded.replace(\"[\", \"\")\n",
    "        decoded = decoded.replace(\"]\", \"\")\n",
    "        # print(f\"Decoded User-Agent: {decoded}\")\n",
    "        tokens = self.tokenizer[\"default\"].tokenize(decoded)\n",
    "        # print(\"Tokens before clean:\", tokens)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = self.clean_tokens(tokens)\n",
    "        # print(\"Tokens after clean:\", tokens)\n",
    "        return tokens\n",
    "    def process_attack(self, value):\n",
    "        \"\"\"\n",
    "        Process user-agent strings to extract tokens.\n",
    "        \"\"\"\n",
    "        # print(f\"User-Agent: {value}\")\n",
    "        decoded = unquote(str(value))\n",
    "        decoded = decoded.replace(\"--\", \" \")\n",
    "        decoded = decoded.replace(\"_\", \" \")\n",
    "        decoded = decoded.replace(\"'\", \"\")\n",
    "        decoded = decoded.replace(\"[\", \"\")\n",
    "        decoded = decoded.replace(\"]\", \"\")\n",
    "        # print(f\"Decoded User-Agent: {decoded}\")\n",
    "        tokens = self.tokenizer[\"attack\"].tokenize(decoded)\n",
    "        # print(\"Tokens before clean:\", tokens)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = self.clean_tokens(tokens)\n",
    "        # print(\"Tokens after clean:\", tokens)\n",
    "        return tokens\n",
    "\n",
    "    def process_cookie(self, value):\n",
    "\n",
    "        # print(f\"Attack Tag: {value}\")\n",
    "        parts = re.split(r'(;)', str(value))\n",
    "        alltokens = []\n",
    "        for part in parts:\n",
    "            # Nếu part chứa dấu \"=\" và dấu \"=\" nằm trước nửa chiều dài của part\n",
    "            if \"=\" in part:\n",
    "                eq_index = part.find(\"=\")\n",
    "                if eq_index < (len(part) / 2):\n",
    "                    # Tách part thành 3 phần: phần trước dấu \"=\", dấu \"=\" và phần sau dấu \"=\"\n",
    "                    first_part, remainder = part.split(\"=\", 1)\n",
    "                    subparts = [first_part, \"=\", remainder]\n",
    "                else:\n",
    "                    subparts = [part]\n",
    "            else:\n",
    "                subparts = [part]\n",
    "            for i, subpart in enumerate(subparts):\n",
    "                # print(f\"Part: {subpart}\")\n",
    "                decoded = unquote(str(subpart))\n",
    "                # Chỉ thực hiện decode base64 đối với phần remainder (index 2)\n",
    "                if i == 2:\n",
    "                    decoded = self.decode_base64(decoded)\n",
    "                decoded = decoded.replace(\"--\", \" \")\n",
    "                decoded = decoded.replace(\"_\", \" \")\n",
    "                decoded = decoded.replace(\"'\", \"\")\n",
    "                decoded = decoded.replace(\"[\", \"\")\n",
    "                decoded = decoded.replace(\"]\", \"\")\n",
    "                # print(f\"Decoded URL: {decoded}\")\n",
    "                tokens = self.tokenizer['attack'].tokenize(decoded)\n",
    "                # print(\"Tokens before clean:\", tokens)\n",
    "                tokens = [token.lower() for token in tokens]\n",
    "                tokens = self.clean_tokens(tokens)\n",
    "                # print(\"Tokens after clean:\", tokens)\n",
    "                alltokens.extend(tokens)\n",
    "        # print(\"All tokens:\", alltokens)\n",
    "        return alltokens\n",
    "\n",
    "    def tokenize_df(self, row, columns,):\n",
    "        \"\"\"\n",
    "        Tokenizes the input value and returns a list of tokens.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for col in columns:\n",
    "            lower_col = col.lower()\n",
    "            if \"cookie\" in lower_col:\n",
    "                tokens += self.process_cookie(row[col])\n",
    "            elif \"user-agent\" in lower_col:\n",
    "                tokens += self.process_user(row[col])\n",
    "            elif \"date\" in lower_col:\n",
    "                tokens += self.process_date(row[col])\n",
    "            elif \"host\" in lower_col:\n",
    "                tokens += self.process_host(row[col])\n",
    "            elif \"content-length\" in lower_col:\n",
    "                tokens += self.process_content_length(row[col])\n",
    "            elif any(x in lower_col for x in [\"sec-fetch-user\", \"accept\", \"sec-fetch-mode\", \"cache-control\", \"connection\",\"sec-ch-ua-mobile\", \"sec-ch-ua-platform\"]):\n",
    "                tokens += self.process_default(row[col])\n",
    "            else:\n",
    "                tokens += self.process_attack(row[col])\n",
    "        return tokens\n",
    "\n",
    "    def tokenize_json(self, json_data):\n",
    "        \"\"\"\n",
    "        Tokenizes the input JSON data and returns a list of tokens.\n",
    "        Processes the 'request' section similarly to how tokenize_df processes a DataFrame row.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        request = json_data.get(\"request\", {})\n",
    "        \n",
    "        # First process the headers if available.\n",
    "        headers = request.get(\"headers\", {})\n",
    "        if isinstance(headers, dict):\n",
    "            for hkey, hvalue in headers.items():\n",
    "                lower_hkey = hkey.lower()\n",
    "                if \"cookie\" in lower_hkey:\n",
    "                    tokens += self.process_cookie(hvalue)\n",
    "                elif \"user-agent\" in lower_hkey:\n",
    "                    tokens += self.process_user(hvalue)\n",
    "                elif \"date\" in lower_hkey:\n",
    "                    tokens += self.process_date(hvalue)\n",
    "                elif \"host\" in lower_hkey:\n",
    "                    tokens += self.process_host(hvalue)\n",
    "                elif \"content-length\" in lower_hkey:\n",
    "                    tokens += self.process_content_length(hvalue)\n",
    "                elif any(x in lower_hkey for x in [\"sec-fetch-user\", \"accept\", \"sec-fetch-mode\", \"cache-control\", \"connection\", \"sec-ch-ua-mobile\", \"sec-ch-ua-platform\"]):\n",
    "                    tokens += self.process_default(hvalue)\n",
    "                else:\n",
    "                    tokens += self.process_attack(hvalue)\n",
    "        elif isinstance(headers, list):\n",
    "            for item in headers:\n",
    "                tokens += self.process_default(item)\n",
    "        \n",
    "        # Process the remaining keys in request (excluding headers)\n",
    "        for key, value in request.items():\n",
    "            if key == \"headers\":\n",
    "                continue\n",
    "            lower_key = key.lower()\n",
    "            if isinstance(value, str):\n",
    "                if \"user-agent\" in lower_key:\n",
    "                    tokens += self.process_user(value)\n",
    "                elif \"date\" in lower_key:\n",
    "                    tokens += self.process_date(value)\n",
    "                elif \"host\" in lower_key:\n",
    "                    tokens += self.process_host(value)\n",
    "                elif \"content-length\" in lower_key:\n",
    "                    tokens += self.process_content_length(value)\n",
    "                elif any(x in lower_key for x in [\"sec-fetch-user\", \"accept\", \"sec-fetch-mode\", \"cache-control\", \"connection\", \"sec-ch-ua-mobile\", \"sec-ch-ua-platform\"]):\n",
    "                    tokens += self.process_default(value)\n",
    "                else:\n",
    "                    tokens += self.process_attack(value)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    tokens += self.process_default(item)\n",
    "                    \n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb0813f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['request.headers.Sec-Ch-Ua-Mobile',\n",
       "       'request.headers.Sec-Ch-Ua-Platform', 'request.headers.Accept',\n",
       "       'request.headers.Connection', 'request.headers.User-Agent',\n",
       "       'request.headers.Cache-Control', 'request.headers.Sec-Fetch-Mode',\n",
       "       'request.headers.Content-Length', 'request.headers.Sec-Fetch-Dest',\n",
       "       'request.headers.Accept-Language', 'request.headers.Sec-Fetch-User',\n",
       "       'request.headers.Date', 'request.headers.Host',\n",
       "       'request.headers.Cookie', 'request.headers.Set-Cookie',\n",
       "       'request.headers.Upgrade-Insecure-Requests',\n",
       "       'request.headers.Accept-Encoding', 'request.headers.Sec-Fetch-Site',\n",
       "       'request.url', 'request.method', 'request.body', 'request.Attack_Tag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf36d72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Normal',\n",
       " 'Directory Traversal',\n",
       " 'SQL Injection',\n",
       " 'XSS',\n",
       " 'Log Forging',\n",
       " 'Cookie Injection',\n",
       " 'RCE',\n",
       " 'LOG4J']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_tags = df['request.Attack_Tag'].unique().tolist()\n",
    "attack_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac16c4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['windows']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = HTokenizer()\n",
    "# Tokenize all rows in the DataFrame\n",
    "token = tokenizer.process_default(\"\\\"Windows\\\"\")\n",
    "token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LTModel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
